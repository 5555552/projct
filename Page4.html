<html>
      <head>
            <title>Page 4</title>
      </head><h1><font color="red" size="7">The future of Artificial Intelligence</font> </h1>
      <body>"
            <center></center>
<h3><font color="black" size="4"></font>On January 11–12, NYU hosted a symposium titled “The Future of Artificial Intelligence,” which brought together artificial intelligence (AI) researchers from academia and industry for two intense days of discussion (http://cds.nyu.edu/ai/?pass=CfLjizw47). In this editorial, I provide some context and perspective on the event—specifically, the major questions facing the field at the current time and the technical and societal challenges involved in addressing them.Google CEO Eric Schmidt gave the opening talk, listing the recent advancements in AI that have enabled capabilities for solving previously intractable problems. He stressed that AI should forge a future that benefits “the many” instead of “the few.” Challenges in water purification, synthetic food production, logistics of distribution, and optimal management of supply/demand of energy are some of the areas where our ability to leverage natural flows of data governs our ability to meet our collective needs.Schmidt envisions a world where research is “open,” contrasted with military-supported research, which is often classified and motivated by considerations other than the good of humanity. He discussed the importance of industry/academic collaboration, for example, by measuring advances on open real-world data sets available to the scientific community at large. He stressed the need for “platforms” for scientific advancement and the need for systems built on these platforms to be able to handle the dynamic nature of problems, and be inventive enough to be able to learn in parallel with their operationFacebook CTO Mike Schroepfer stressed a similar commitment to “10 year problems” and sharing data and algorithms, promoting open and peer-reviewed research. Facebook is eager to promote basic research by “installing labs next to where the scientists are.”

      The discussion among the scientists at the symposium was forward looking, projecting impacts and benefits, and humanity's longer-term future with AI. Five major questions emerged from this discussion:.. Why is it different this time? The history of AI has seen several “boom–bust” cycles, where the optimism was driven by some perceived significant advance, followed by disappointment due to unrealistic expectations. We are currently in another boom. Is it different this time? If so, why?

      2. How should we control systems that are potentially more intelligent than humans, whoseworking we don't fully understand? A version of this question that is more pressing is how should we control systems that are extremely complex and potentially more accurate than humans but can't directly explain their own behavior?*

      3. Should there be an objective function for AI systems or is diversity more appropriate?
      
      4. Are we likely to see ourselves replaced by robots for most tasks or augmented by machine intelligence? In the process, will AI create more jobs than it will destroy, or the other way around?5. Is our current regulatory framework for governing the rights and actions of humans adequate for dealing with robots?  </h3>
<h4> <font color="green" size="6">Why Is It Different This Time?</font> </h4>
<h5> <font color="black" size="4">One fundamental and measurable difference between now and the past is that machine learning has helped us come a long way toward solving perception. Machines were unable to read, hear, or see, which typically required the input to be curated for them. Newer systems can take the visual, auditory, or language input directly. This advancement enables the machine to take direct inputs from the world without human involvement and create its own internal representation for further processing.

      Big data is another difference, along with powerful tools, especially in the area of supervised learningwhere the machine learns from data represented in input–output pairs. Many problems are amenable to this type of formulation, and we have witnessed a mushrooming of machine learning systems in virtually every domain where large data sets have become available. It is becoming more common for computers to perform tasks better than the best humans can.

      However, for many problems, clean input–output pairs are unavailable, and/or supervised learning is not feasible. For example, we may often care about some cumulative outcome such as overall health or profit/cost instead of the goodness of every decision along the way to the final outcome. This may favora different type of formulation, such as a “reinforcement learning” one, in which rewards accrue occasionally, cumulative benefit is maximized, and credit assignment is difficult. </font></h5>
      </body>
</html>